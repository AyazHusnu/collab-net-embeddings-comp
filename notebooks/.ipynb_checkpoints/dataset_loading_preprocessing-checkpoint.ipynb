{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d2dc3ca-2ed0-4a5a-b385-4f5efc15f5cd",
   "metadata": {},
   "source": [
    "# Collaboration Networks — Data Loading + Graph Summary + LCC\n",
    "\n",
    "This notebook:\n",
    "\n",
    "- Loads `.mtx` adjacency matrices and converts them to NetworkX graphs\n",
    "- Prints key statistics (Nodes, Edges, Avg Degree, Avg Clustering, Approx Avg Path Length)\n",
    "- Extracts the Largest Connected Component (LCC)\n",
    "- Compares metrics before vs after LCC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1e9355a-c576-45c9-8796-7048d07ecd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "from scipy.io import mmread\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "pd.set_option(\"display.width\", 160)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "187dfccc-95d1-4c6a-af6a-5d897c913134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset paths OK\n"
     ]
    }
   ],
   "source": [
    "DATASETS = {\n",
    "    \"ca-CondMat\": \"../data/ca-CondMat/ca-CondMat.mtx\",\n",
    "    \"ca-GrQc\":    \"../data/ca-GrQc/ca-GrQc.mtx\",\n",
    "    \"ca-HepPh\":   \"../data/ca-HepPh/ca-HepPh.mtx\",\n",
    "}\n",
    "# Basic sanity check\n",
    "missing = [name for name, path in DATASETS.items() if not os.path.exists(path)]\n",
    "if missing:\n",
    "    raise FileNotFoundError(f\"Missing dataset(s): {missing}\\nPlease check the file paths.\")\n",
    "print(\"Dataset paths OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83ed9e58-b432-4eda-852c-35dfb97f94fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph_from_mtx(path: str) -> tuple[nx.Graph, float]:\n",
    "    \"\"\"\n",
    "    Reads a Matrix Market (.mtx) adjacency matrix and builds an undirected simple graph.\n",
    "    - Converts to CSR for efficiency\n",
    "    - Builds a NetworkX Graph from the sparse matrix\n",
    "    - Removes self-loops\n",
    "    - Ensures a simple undirected Graph\n",
    "    Returns: (Graph, load_time_seconds)\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    A = mmread(path)\n",
    "\n",
    "    if not isinstance(A, csr_matrix):\n",
    "        A = A.tocsr()\n",
    "\n",
    "    G = nx.from_scipy_sparse_array(A)  # Graph for symmetric adjacency\n",
    "\n",
    "    # Remove self-loops if any\n",
    "    G.remove_edges_from(nx.selfloop_edges(G))\n",
    "\n",
    "    # Ensure simple Graph\n",
    "    if isinstance(G, (nx.MultiGraph, nx.MultiDiGraph)):\n",
    "        G = nx.Graph(G)\n",
    "\n",
    "    return G, (time.time() - t0)\n",
    "\n",
    "\n",
    "def get_lcc(G: nx.Graph) -> nx.Graph:\n",
    "    \"\"\"Return the Largest Connected Component (induced subgraph) as a copy.\"\"\"\n",
    "    if G.number_of_nodes() == 0:\n",
    "        return G.copy()\n",
    "    largest_cc_nodes = max(nx.connected_components(G), key=len)\n",
    "    return G.subgraph(largest_cc_nodes).copy()\n",
    "\n",
    "\n",
    "def approx_average_path_length(G: nx.Graph, sample_size: int = 60, cutoff: int = None) -> float:\n",
    "    \"\"\"\n",
    "    Approximate average shortest path length by sampling BFS from random source nodes.\n",
    "    For disconnected graphs, we average over reachable pairs from sampled sources.\n",
    "    Returns NaN if graph is too small or no reachable pairs are found.\n",
    "    \"\"\"\n",
    "    n = G.number_of_nodes()\n",
    "    if n < 2:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    nodes = list(G.nodes())\n",
    "    k = min(sample_size, n)\n",
    "    sources = random.sample(nodes, k)\n",
    "\n",
    "    dists = []\n",
    "    for s in sources:\n",
    "        sp = nx.single_source_shortest_path_length(G, s, cutoff=cutoff)\n",
    "        for _, dist in sp.items():\n",
    "            if dist > 0:\n",
    "                dists.append(dist)\n",
    "\n",
    "    return float(np.mean(dists)) if dists else float(\"nan\")\n",
    "\n",
    "\n",
    "def graph_metrics(G: nx.Graph, path_sample_size: int = 60) -> dict:\n",
    "    \"\"\"\n",
    "    Compute requested graph-level stats:\n",
    "    - Nodes, Edges\n",
    "    - Average Degree\n",
    "    - Average Clustering\n",
    "    - Approx. Average Shortest Path Length\n",
    "    \"\"\"\n",
    "    n = G.number_of_nodes()\n",
    "    m = G.number_of_edges()\n",
    "\n",
    "    avg_degree = (2.0 * m / n) if n > 0 else float(\"nan\")\n",
    "    avg_clustering = nx.average_clustering(G) if n > 0 else float(\"nan\")\n",
    "    avg_path_approx = approx_average_path_length(G, sample_size=path_sample_size)\n",
    "\n",
    "    return {\n",
    "        \"Nodes\": n,\n",
    "        \"Edges\": m,\n",
    "        \"Average Degree\": avg_degree,\n",
    "        \"Average Clustering\": avg_clustering,\n",
    "        \"Average Path (approx)\": avg_path_approx,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d5be8ee-a9c4-4c94-90bc-0a88692d5045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ca-CondMat: loaded in 0.36s | nodes=21,363 | edges=91,286\n",
      "ca-GrQc: loaded in 0.03s | nodes=4,158 | edges=13,422\n",
      "ca-HepPh: loaded in 0.28s | nodes=11,204 | edges=117,619\n"
     ]
    }
   ],
   "source": [
    "graphs = {}\n",
    "load_times = {}\n",
    "\n",
    "for name, path in DATASETS.items():\n",
    "    G, load_s = load_graph_from_mtx(path)\n",
    "    graphs[name] = G\n",
    "    load_times[name] = load_s\n",
    "    print(f\"{name}: loaded in {load_s:.2f}s | nodes={G.number_of_nodes():,} | edges={G.number_of_edges():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "865b6744-67bf-4496-8453-63d01b6b36f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nodes</th>\n",
       "      <th>Edges</th>\n",
       "      <th>Average Degree</th>\n",
       "      <th>Average Clustering</th>\n",
       "      <th>Average Path (approx)</th>\n",
       "      <th>Load Time (s)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ca-CondMat</th>\n",
       "      <td>21363</td>\n",
       "      <td>91286</td>\n",
       "      <td>8.546178</td>\n",
       "      <td>0.641732</td>\n",
       "      <td>5.176290</td>\n",
       "      <td>0.357375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ca-GrQc</th>\n",
       "      <td>4158</td>\n",
       "      <td>13422</td>\n",
       "      <td>6.455988</td>\n",
       "      <td>0.556878</td>\n",
       "      <td>6.065243</td>\n",
       "      <td>0.025306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ca-HepPh</th>\n",
       "      <td>11204</td>\n",
       "      <td>117619</td>\n",
       "      <td>20.995894</td>\n",
       "      <td>0.621582</td>\n",
       "      <td>4.643917</td>\n",
       "      <td>0.283374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Nodes   Edges  Average Degree  Average Clustering  Average Path (approx)  Load Time (s)\n",
       "Dataset                                                                                            \n",
       "ca-CondMat  21363   91286        8.546178            0.641732               5.176290       0.357375\n",
       "ca-GrQc      4158   13422        6.455988            0.556878               6.065243       0.025306\n",
       "ca-HepPh    11204  117619       20.995894            0.621582               4.643917       0.283374"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_before = []\n",
    "for name, G in graphs.items():\n",
    "    stats = graph_metrics(G, path_sample_size=60)\n",
    "    stats[\"Dataset\"] = name\n",
    "    stats[\"Load Time (s)\"] = load_times[name]\n",
    "    rows_before.append(stats)\n",
    "\n",
    "df_before = pd.DataFrame(rows_before).set_index(\"Dataset\")\n",
    "df_before\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3eb28068-bca6-4795-9b8d-42eaab333a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ca-CondMat: LCC nodes=21,363 (100.00%) | edges=91,286 (100.00%)\n",
      "ca-GrQc: LCC nodes=4,158 (100.00%) | edges=13,422 (100.00%)\n",
      "ca-HepPh: LCC nodes=11,204 (100.00%) | edges=117,619 (100.00%)\n"
     ]
    }
   ],
   "source": [
    "graphs_lcc = {}\n",
    "lcc_coverage = {}\n",
    "\n",
    "for name, G in graphs.items():\n",
    "    Glcc = get_lcc(G)\n",
    "    graphs_lcc[name] = Glcc\n",
    "\n",
    "    lcc_coverage[name] = {\n",
    "        \"LCC Node %\": 100.0 * Glcc.number_of_nodes() / max(1, G.number_of_nodes()),\n",
    "        \"LCC Edge %\": 100.0 * Glcc.number_of_edges() / max(1, G.number_of_edges()),\n",
    "    }\n",
    "\n",
    "    print(\n",
    "        f\"{name}: LCC nodes={Glcc.number_of_nodes():,} ({lcc_coverage[name]['LCC Node %']:.2f}%) | \"\n",
    "        f\"edges={Glcc.number_of_edges():,} ({lcc_coverage[name]['LCC Edge %']:.2f}%)\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94342bbf-6414-4c15-a969-f2bed05af19a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nodes</th>\n",
       "      <th>Edges</th>\n",
       "      <th>Average Degree</th>\n",
       "      <th>Average Clustering</th>\n",
       "      <th>Average Path (approx)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ca-CondMat</th>\n",
       "      <td>21363</td>\n",
       "      <td>91286</td>\n",
       "      <td>8.546178</td>\n",
       "      <td>0.641732</td>\n",
       "      <td>5.280860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ca-GrQc</th>\n",
       "      <td>4158</td>\n",
       "      <td>13422</td>\n",
       "      <td>6.455988</td>\n",
       "      <td>0.556878</td>\n",
       "      <td>5.942543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ca-HepPh</th>\n",
       "      <td>11204</td>\n",
       "      <td>117619</td>\n",
       "      <td>20.995894</td>\n",
       "      <td>0.621582</td>\n",
       "      <td>4.580118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Nodes   Edges  Average Degree  Average Clustering  Average Path (approx)\n",
       "Dataset                                                                             \n",
       "ca-CondMat  21363   91286        8.546178            0.641732               5.280860\n",
       "ca-GrQc      4158   13422        6.455988            0.556878               5.942543\n",
       "ca-HepPh    11204  117619       20.995894            0.621582               4.580118"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_after = []\n",
    "for name, G in graphs_lcc.items():\n",
    "    stats = graph_metrics(G, path_sample_size=60)\n",
    "    stats[\"Dataset\"] = name\n",
    "    rows_after.append(stats)\n",
    "\n",
    "df_after = pd.DataFrame(rows_after).set_index(\"Dataset\")\n",
    "df_after\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a65585-864b-49e9-8e82-91508598db64",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- In all three datasets, the original graphs are already connected; therefore, extracting the Largest Connected Component (LCC) does not change the number of nodes or edges.\n",
    "- Despite the graphs being connected, computing the exact average shortest path length is computationally expensive for large networks.\n",
    "- For this reason, **Average Path (approx)** is estimated by sampling BFS from randomly selected source nodes and averaging distances over reachable node pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e25c539-fb85-4db2-b20d-ae5893ea0b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def edge_split(G, train_ratio=0.8, val_ratio=0.1, seed=42):\n",
    "    \"\"\"\n",
    "    Topology-based edge split for link prediction.\n",
    "    Returns train_edges, val_edges, test_edges\n",
    "    \"\"\"\n",
    "    edges = list(G.edges())\n",
    "    \n",
    "    # First split: train vs (val+test)\n",
    "    train_edges, temp_edges = train_test_split(\n",
    "        edges,\n",
    "        train_size=train_ratio,\n",
    "        random_state=seed,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Second split: val vs test\n",
    "    val_size = val_ratio / (1.0 - train_ratio)\n",
    "    val_edges, test_edges = train_test_split(\n",
    "        temp_edges,\n",
    "        train_size=val_size,\n",
    "        random_state=seed,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    return train_edges, val_edges, test_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a5a33c2-8646-46df-8801-ed48f48ce440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ca-CondMat | train: 73028 | val: 9129 | test: 9129\n",
      "ca-GrQc | train: 10737 | val: 1342 | test: 1343\n",
      "ca-HepPh | train: 94095 | val: 11762 | test: 11762\n"
     ]
    }
   ],
   "source": [
    "splits = {}\n",
    "\n",
    "for name, G in graphs_lcc.items():\n",
    "    train_e, val_e, test_e = edge_split(G)\n",
    "    \n",
    "    splits[name] = {\n",
    "        \"train\": train_e,\n",
    "        \"val\": val_e,\n",
    "        \"test\": test_e\n",
    "    }\n",
    "    \n",
    "    print(\n",
    "        f\"{name} | \"\n",
    "        f\"train: {len(train_e)} | \"\n",
    "        f\"val: {len(val_e)} | \"\n",
    "        f\"test: {len(test_e)}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a41264e6-09e8-4b0f-b62b-00a3570f57fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graphs = {}\n",
    "\n",
    "for name, G in graphs_lcc.items():\n",
    "    G_train = nx.Graph()\n",
    "    G_train.add_nodes_from(G.nodes())\n",
    "    G_train.add_edges_from(splits[name][\"train\"])\n",
    "    train_graphs[name] = G_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9c3a739-1c43-4354-a107-86b703a4f647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ca-CondMat ===\n",
      "Train: [(1035, 3544), (10432, 16337), (1342, 9277), (9056, 19687), (2206, 18788)]\n",
      "Val  : [(9623, 14998), (7033, 10134), (8674, 9370), (378, 13426), (7175, 13412)]\n",
      "Test : [(13816, 21065), (669, 21158), (12954, 14843), (2578, 9153), (2093, 12900)]\n",
      "\n",
      "=== ca-GrQc ===\n",
      "Train: [(568, 3043), (1498, 3625), (1014, 1137), (1471, 1596), (2320, 3027)]\n",
      "Val  : [(1064, 3728), (3074, 3283), (1524, 3658), (2475, 2656), (1188, 3903)]\n",
      "Test : [(1901, 2677), (2394, 2519), (761, 1015), (2049, 2962), (2091, 2168)]\n",
      "\n",
      "=== ca-HepPh ===\n",
      "Train: [(2252, 4334), (4926, 7355), (2284, 4567), (3609, 4184), (6179, 10191)]\n",
      "Val  : [(2222, 4520), (8193, 8863), (4217, 4882), (721, 5779), (3796, 9017)]\n",
      "Test : [(683, 6426), (7244, 9788), (44, 5548), (1158, 9016), (2981, 9436)]\n"
     ]
    }
   ],
   "source": [
    "print(\"=== ca-CondMat ===\")\n",
    "print(\"Train:\", splits[\"ca-CondMat\"][\"train\"][:5])\n",
    "print(\"Val  :\", splits[\"ca-CondMat\"][\"val\"][:5])\n",
    "print(\"Test :\", splits[\"ca-CondMat\"][\"test\"][:5])\n",
    "\n",
    "print(\"\\n=== ca-GrQc ===\")\n",
    "print(\"Train:\", splits[\"ca-GrQc\"][\"train\"][:5])\n",
    "print(\"Val  :\", splits[\"ca-GrQc\"][\"val\"][:5])\n",
    "print(\"Test :\", splits[\"ca-GrQc\"][\"test\"][:5])\n",
    "\n",
    "print(\"\\n=== ca-HepPh ===\")\n",
    "print(\"Train:\", splits[\"ca-HepPh\"][\"train\"][:5])\n",
    "print(\"Val  :\", splits[\"ca-HepPh\"][\"val\"][:5])\n",
    "print(\"Test :\", splits[\"ca-HepPh\"][\"test\"][:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5139fbd7-2922-4dee-894c-ee653af3a7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def canon_edge(u, v):\n",
    "    return (u, v) if u <= v else (v, u)\n",
    "\n",
    "def sample_negative_edges(G, num_samples, forbidden_edges, seed=42, max_tries=10_000_000):\n",
    "    \"\"\"\n",
    "    Samples 'num_samples' negative edges (u,v) such that:\n",
    "    - (u,v) not in forbidden_edges\n",
    "    - (u,v) not in G edges (if you pass all positives in forbidden_edges, this is already ensured)\n",
    "    - u != v\n",
    "    Works for undirected graphs.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    nodes = np.array(list(G.nodes()))\n",
    "    n = len(nodes)\n",
    "\n",
    "    negatives = set()\n",
    "    tries = 0\n",
    "\n",
    "    while len(negatives) < num_samples and tries < max_tries:\n",
    "        u = int(nodes[rng.integers(0, n)])\n",
    "        v = int(nodes[rng.integers(0, n)])\n",
    "        if u == v:\n",
    "            tries += 1\n",
    "            continue\n",
    "        e = canon_edge(u, v)\n",
    "        if e in forbidden_edges:\n",
    "            tries += 1\n",
    "            continue\n",
    "        negatives.add(e)\n",
    "        tries += 1\n",
    "\n",
    "    if len(negatives) < num_samples:\n",
    "        raise RuntimeError(f\"Could only sample {len(negatives)} negatives out of {num_samples}. Try lowering num_samples.\")\n",
    "\n",
    "    return list(negatives)\n",
    "\n",
    "\n",
    "def build_negative_splits(G, splits_for_dataset, neg_ratio=1.0, seed=42):\n",
    "    \"\"\"\n",
    "    Creates negative edges for train/val/test consistent with split.\n",
    "    neg_ratio=1.0 means #neg = #pos for each split.\n",
    "    \"\"\"\n",
    "    train_pos = [canon_edge(*e) for e in splits_for_dataset[\"train\"]]\n",
    "    val_pos   = [canon_edge(*e) for e in splits_for_dataset[\"val\"]]\n",
    "    test_pos  = [canon_edge(*e) for e in splits_for_dataset[\"test\"]]\n",
    "\n",
    "    all_pos = set(train_pos) | set(val_pos) | set(test_pos)\n",
    "\n",
    "    n_train = int(len(train_pos) * neg_ratio)\n",
    "    n_val   = int(len(val_pos)   * neg_ratio)\n",
    "    n_test  = int(len(test_pos)  * neg_ratio)\n",
    "\n",
    "    # Important: forbid ALL positive edges across ALL splits\n",
    "    train_neg = sample_negative_edges(G, n_train, forbidden_edges=all_pos, seed=seed+1)\n",
    "    val_neg   = sample_negative_edges(G, n_val,   forbidden_edges=all_pos | set(train_neg), seed=seed+2)\n",
    "    test_neg  = sample_negative_edges(G, n_test,  forbidden_edges=all_pos | set(train_neg) | set(val_neg), seed=seed+3)\n",
    "\n",
    "    return {\"train_neg\": train_neg, \"val_neg\": val_neg, \"test_neg\": test_neg}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "443b2dbd-9f41-46de-9f9e-9d6cdb91fbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_negative_samples(G, splits_for_dataset, negs_for_dataset):\n",
    "    train_pos = set(canon_edge(*e) for e in splits_for_dataset[\"train\"])\n",
    "    val_pos   = set(canon_edge(*e) for e in splits_for_dataset[\"val\"])\n",
    "    test_pos  = set(canon_edge(*e) for e in splits_for_dataset[\"test\"])\n",
    "    all_pos   = train_pos | val_pos | test_pos\n",
    "\n",
    "    train_neg = set(negs_for_dataset[\"train_neg\"])\n",
    "    val_neg   = set(negs_for_dataset[\"val_neg\"])\n",
    "    test_neg  = set(negs_for_dataset[\"test_neg\"])\n",
    "\n",
    "    # 1) Negatives must not overlap with ANY positives (leakage/contradiction check)\n",
    "    print(\"neg ∩ all_pos (train):\", len(train_neg & all_pos))\n",
    "    print(\"neg ∩ all_pos (val)  :\", len(val_neg & all_pos))\n",
    "    print(\"neg ∩ all_pos (test) :\", len(test_neg & all_pos))\n",
    "\n",
    "    # 2) Negatives should not overlap across splits (optional but recommended)\n",
    "    print(\"train_neg ∩ val_neg :\", len(train_neg & val_neg))\n",
    "    print(\"train_neg ∩ test_neg:\", len(train_neg & test_neg))\n",
    "    print(\"val_neg   ∩ test_neg:\", len(val_neg & test_neg))\n",
    "\n",
    "    # 3) No self-loops\n",
    "    print(\"self-loops (train_neg):\", sum(1 for u,v in train_neg if u==v))\n",
    "    print(\"self-loops (val_neg)  :\", sum(1 for u,v in val_neg if u==v))\n",
    "    print(\"self-loops (test_neg) :\", sum(1 for u,v in test_neg if u==v))\n",
    "\n",
    "    # 4) Not accidentally existing as edges in G (extra safety)\n",
    "    existing = set(canon_edge(u,v) for u,v in G.edges())\n",
    "    print(\"neg ∩ G.edges (train):\", len(train_neg & existing))\n",
    "    print(\"neg ∩ G.edges (val)  :\", len(val_neg & existing))\n",
    "    print(\"neg ∩ G.edges (test) :\", len(test_neg & existing))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06968df6-c96e-471c-98d0-0714f41dfb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg sizes: 73028 9129 9129\n",
      "neg ∩ all_pos (train): 0\n",
      "neg ∩ all_pos (val)  : 0\n",
      "neg ∩ all_pos (test) : 0\n",
      "train_neg ∩ val_neg : 0\n",
      "train_neg ∩ test_neg: 0\n",
      "val_neg   ∩ test_neg: 0\n",
      "self-loops (train_neg): 0\n",
      "self-loops (val_neg)  : 0\n",
      "self-loops (test_neg) : 0\n",
      "neg ∩ G.edges (train): 0\n",
      "neg ∩ G.edges (val)  : 0\n",
      "neg ∩ G.edges (test) : 0\n",
      "\n",
      "First 10 train negatives: [(5969, 18298), (3129, 3719), (586, 3656), (785, 7249), (20258, 21086), (6222, 9884), (6516, 18467), (4498, 13122), (1793, 8759), (3424, 5330)]\n"
     ]
    }
   ],
   "source": [
    "dataset = \"ca-CondMat\"  # ca-GrQc, ca-HepPh\n",
    "\n",
    "G = graphs_lcc[dataset]\n",
    "\n",
    "negs = build_negative_splits(G, splits[dataset], neg_ratio=1.0, seed=42)\n",
    "print(\"Neg sizes:\",\n",
    "      len(negs[\"train_neg\"]), len(negs[\"val_neg\"]), len(negs[\"test_neg\"]))\n",
    "\n",
    "check_negative_samples(G, splits[dataset], negs)\n",
    "\n",
    "print(\"\\nFirst 10 train negatives:\", negs[\"train_neg\"][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d8504af-9048-48db-90e9-dbe554a83d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ca-CondMat counts:\n",
      "Train: pos=73,028 neg=73,028\n",
      "Val  : pos=9,129 neg=9,129\n",
      "Test : pos=9,129 neg=9,129\n"
     ]
    }
   ],
   "source": [
    "dataset = \"ca-CondMat\"\n",
    "\n",
    "train_pos = splits[dataset][\"train\"]\n",
    "val_pos   = splits[dataset][\"val\"]\n",
    "test_pos  = splits[dataset][\"test\"]\n",
    "\n",
    "train_neg = negs[\"train_neg\"]\n",
    "val_neg   = negs[\"val_neg\"]\n",
    "test_neg  = negs[\"test_neg\"]\n",
    "\n",
    "print(f\"{dataset} counts:\")\n",
    "print(f\"Train: pos={len(train_pos):,} neg={len(train_neg):,}\")\n",
    "print(f\"Val  : pos={len(val_pos):,} neg={len(val_neg):,}\")\n",
    "print(f\"Test : pos={len(test_pos):,} neg={len(test_neg):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17fab4e1-0415-4a53-bb73-9b8fb1fea70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_edge_dataset(pos_edges, neg_edges):\n",
    "    X = pos_edges + neg_edges\n",
    "    y = [1]*len(pos_edges) + [0]*len(neg_edges)\n",
    "    return X, np.array(y)\n",
    "\n",
    "X_test, y_test = make_edge_dataset(test_pos, test_neg)\n",
    "X_val, y_val   = make_edge_dataset(val_pos, val_neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0de0a234-9a00-44c5-a723-cea4e56a007c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ca-CondMat | VAL: AUC=0.8877  AP=0.9060\n",
      "ca-CondMat | TEST: AUC=0.8822  AP=0.9022\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "# ---- choose dataset ----\n",
    "dataset = \"ca-CondMat\"  # change to \"ca-GrQc\" or \"ca-HepPh\"\n",
    "\n",
    "G_train = train_graphs[dataset]\n",
    "\n",
    "# positives / negatives for evaluation\n",
    "train_pos = splits[dataset][\"train\"]\n",
    "val_pos   = splits[dataset][\"val\"]\n",
    "test_pos  = splits[dataset][\"test\"]\n",
    "\n",
    "train_neg = negs[\"train_neg\"]\n",
    "val_neg   = negs[\"val_neg\"]\n",
    "test_neg  = negs[\"test_neg\"]\n",
    "\n",
    "# ---- build node index mapping from training graph ----\n",
    "nodes = list(G_train.nodes())\n",
    "node2idx = {n:i for i, n in enumerate(nodes)}\n",
    "n = len(nodes)\n",
    "\n",
    "# ---- build sparse adjacency of TRAIN graph ----\n",
    "rows, cols = [], []\n",
    "for u, v in G_train.edges():\n",
    "    i, j = node2idx[u], node2idx[v]\n",
    "    rows += [i, j]\n",
    "    cols += [j, i]\n",
    "A = sp.csr_matrix((np.ones(len(rows)), (rows, cols)), shape=(n, n))\n",
    "\n",
    "# ---- SVD embedding (dimension k) ----\n",
    "k = 64  # you can try 32/64/128\n",
    "u, s, vt = svds(A, k=k)             # u:(n,k), s:(k,), vt:(k,n)\n",
    "order = np.argsort(-s)              # sort by descending singular values\n",
    "u, s = u[:, order], s[order]\n",
    "Z = u * np.sqrt(s)                  # node embeddings: (n,k)\n",
    "\n",
    "def edge_score(edges):\n",
    "    # dot product similarity\n",
    "    scores = []\n",
    "    for a, b in edges:\n",
    "        ia, ib = node2idx[a], node2idx[b]\n",
    "        scores.append(float(np.dot(Z[ia], Z[ib])))\n",
    "    return np.array(scores)\n",
    "\n",
    "def eval_split(pos_edges, neg_edges, split_name=\"\"):\n",
    "    y_true = np.array([1]*len(pos_edges) + [0]*len(neg_edges))\n",
    "    y_score = np.concatenate([edge_score(pos_edges), edge_score(neg_edges)])\n",
    "    auc = roc_auc_score(y_true, y_score)\n",
    "    ap  = average_precision_score(y_true, y_score)\n",
    "    print(f\"{dataset} | {split_name}: AUC={auc:.4f}  AP={ap:.4f}\")\n",
    "\n",
    "eval_split(val_pos,  val_neg,  \"VAL\")\n",
    "eval_split(test_pos, test_neg, \"TEST\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61166710-0127-4e2c-b7d9-1bf7fa22a7a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
